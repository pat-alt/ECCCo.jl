```{julia}
include("notebooks/setup.jl")
eval(setup_notebooks)
```

# MNIST

## Anecdotal Evidence

### Examples in Introduction

#### Wachter and JSMA

```{julia}
Random.seed!(1234)

# Data:
counterfactual_data = load_mnist()
X, y = CounterfactualExplanations.DataPreprocessing.unpack_data(counterfactual_data)
input_dim, n_obs = size(counterfactual_data.X)
M = load_mnist_mlp()

# Target:
factual_label = 8
x_factual = reshape(X[:,rand(findall(predict_label(M, counterfactual_data).==factual_label))],input_dim,1)
target = 3
factual = predict_label(M, counterfactual_data, x_factual)[1]
纬 = 0.9

# Training params:
T = 100
opt = Flux.Optimise.Adam(0.01)
```

```{julia}
# Search:
generator = GenericGenerator(opt=opt)
ce_wachter = generate_counterfactual(
    x_factual, target, counterfactual_data, M, generator; 
    decision_threshold=纬, max_iter=T,
    initialization=:identity,
)
generator = GreedyGenerator(畏=1.0)
ce_jsma = generate_counterfactual(
    x_factual, target, counterfactual_data, M, generator; 
    decision_threshold=纬, max_iter=T,
    initialization=:identity,
)
```

```{julia}
p1 = Plots.plot(
    convert2image(MNIST, reshape(x_factual,28,28)),
    axis=nothing, 
    size=(img_height, img_height),
    title="Factual"
)
plts = [p1]

ces = zip([ce_wachter,ce_jsma])
counterfactuals = reduce((x,y)->cat(x,y,dims=3),map(ce -> CounterfactualExplanations.counterfactual(ce[1]), ces))
phat = reduce((x,y) -> cat(x,y,dims=3), map(ce -> target_probs(ce[1]), ces))
for x in zip(eachslice(counterfactuals; dims=3), eachslice(phat; dims=3), ["Wachter","JSMA"])
    ce, _phat, _name = (x[1],x[2],x[3])
    _title = "$(_name) (p=$(round(_phat[1]; digits=2)))"
    plt = Plots.plot(
        convert2image(MNIST, reshape(ce,28,28)),
        axis=nothing, 
        size=(img_height, img_height),
        title=_title
    )
    plts = [plts..., plt]
end
plt = Plots.plot(plts...; size=(img_height*length(plts),img_height), layout=(1,length(plts)))
display(plt)
savefig(plt, joinpath(output_images_path, "you_may_not_like_it.png"))
```

#### REVISE

```{julia}
using CounterfactualExplanations.Models: load_mnist_vae
vae = load_mnist_vae()
vae_weak = load_mnist_vae(;strong=false)
Serialization.serialize(joinpath(output_path,"mnist_classifier.jls"), M)
Serialization.serialize(joinpath(output_path,"mnist_vae.jls"), vae)
Serialization.serialize(joinpath(output_path,"mnist_vae_weak.jls"), vae_weak)
```

```{julia}
# Define generator:
generator = REVISEGenerator(
  opt = opt,
  位=0.1
)
# Generate recourse:
counterfactual_data.generative_model = vae # assign generative model
ce_strong = generate_counterfactual(
    x_factual, target, counterfactual_data, M, generator; 
    decision_threshold=纬, max_iter=T,
    initialization=:identity,
    converge_when=:generator_conditions,
)
counterfactual_data = deepcopy(counterfactual_data)
counterfactual_data.generative_model = vae_weak
ce_weak = generate_counterfactual(
    x_factual, target, counterfactual_data, M, generator;
    decision_threshold=纬, max_iter=T,
    initialization=:identity,
    converge_when=:generator_conditions,
)
```

```{julia}
ces = zip([ce_strong,ce_weak])
counterfactuals = reduce((x,y)->cat(x,y,dims=3),map(ce -> CounterfactualExplanations.counterfactual(ce[1]), ces))
phat = reduce((x,y) -> cat(x,y,dims=3), map(ce -> target_probs(ce[1]), ces))
plts = [p1]
for x in zip(eachslice(counterfactuals; dims=3), eachslice(phat; dims=3), ["Strong VAE","Weak VAE"])
    ce, _phat, _name = (x[1],x[2],x[3])
    _title = "$(_name) (p=$(round(_phat[1]; digits=2)))"
    plt = Plots.plot(
        convert2image(MNIST, reshape(ce,28,28)),
        axis=nothing, 
        size=(img_height, img_height),
        title=_title
    )
    plts = [plts..., plt]
end
plt = Plots.plot(plts...; size=(img_height*length(plts),img_height), layout=(1,length(plts)))
display(plt)
savefig(plt, joinpath(output_images_path, "surrogate_gone_wrong.png"))
```

### ECCCo

```{julia}
function pre_process(x; noise::Float32=0.03f0)
    系 = Float32.(randn(size(x)) * noise)
    x = @.(2 * x - 1) .+ 系
    return x
end
```

```{julia}
# Hyper:
_retrain = false
_regen = false

# Data:
n_obs = 10000
counterfactual_data = load_mnist(n_obs)
counterfactual_data.X = pre_process.(counterfactual_data.X)
X, y = CounterfactualExplanations.DataPreprocessing.unpack_data(counterfactual_data)
X = table(permutedims(X))
x_factual = reshape(pre_process(x_factual, noise=0.0f0), input_dim, 1)
labels = counterfactual_data.output_encoder.labels
input_dim, n_obs = size(counterfactual_data.X)
n_digits = Int(sqrt(input_dim))
output_dim = length(unique(labels))
```

First, let's create a couple of image classifier architectures:

```{julia}
# Model parameters:
epochs = 100
batch_size = minimum([Int(round(n_obs/10)), 128])
n_hidden = 128
activation = Flux.relu
builder = MLJFlux.@builder Flux.Chain(
    Dense(n_in, n_hidden, activation),
    Dense(n_hidden, n_out),
)
n_ens = 5                                   # number of models in ensemble
_loss = Flux.Losses.logitcrossentropy       # loss function
_finaliser = x -> x                         # finaliser function
```

```{julia}
# JEM parameters:
x = Uniform(-1,1)
y = Categorical(ones(output_dim) ./ output_dim)
sampler = ConditionalSampler(
    x, y, 
    input_size=(input_dim,), 
    batch_size=1,
)
伪 = [1.0,1.0,1e-2]      # penalty strengths
```

```{julia}
# Simple MLP:
mlp = NeuralNetworkClassifier(
    builder=builder, 
    epochs=epochs,
    batch_size=batch_size,
    finaliser=_finaliser,
    loss=_loss,
)

# Deep Ensemble:
mlp_ens = EnsembleModel(model=mlp, n=n_ens)

# Joint Energy Model:
jem = JointEnergyClassifier(
    sampler;
    builder=builder,
    epochs=epochs,
    batch_size=batch_size,
    finaliser=_finaliser,
    loss=_loss,
    jem_training_params=(
        伪=伪,verbosity=10,
    ),
    sampling_steps=20,
)

# JEM with adversarial training:
jem_adv = deepcopy(jem)
# jem_adv.adv_training = true

# Deep Ensemble of Joint Energy Models:
jem_ens = EnsembleModel(model=jem, n=n_ens)

# Deep Ensemble of Joint Energy Models with adversarial training:
# jem_ens_plus = EnsembleModel(model=jem_adv, n=n_ens)

# Dictionary of models:
models = Dict(
    "MLP" => mlp,
    "MLP Ensemble" => mlp_ens,
    "JEM" => jem,
    "JEM Ensemble" => jem_ens,
    # "JEM Ensemble+" => jem_ens_plus,
)
```


```{julia}
# Train models:
function _train(model, X=X, y=labels; cov=.95, method=:simple_inductive, mod_name="model")
    conf_model = conformal_model(model; method=method, coverage=cov)
    mach = machine(conf_model, X, y)
    @info "Begin training $mod_name."
    fit!(mach)
    @info "Finished training $mod_name."
    M = ECCCo.ConformalModel(mach.model, mach.fitresult)
    return M
end
if _retrain
    model_dict = Dict(mod_name => _train(mod; mod_name=mod_name) for (mod_name, mod) in models)
    Serialization.serialize(joinpath(output_path,"mnist_models.jls"), model_dict)
else
    model_dict = Serialization.deserialize(joinpath(output_path,"mnist_models.jls"))
end
```

```{julia}
# Plot generated samples:
if _regen 
    for (mod_name, mod) in model_dict
        if ECCCo._has_sampler(mod)
            sampler = ECCCo._get_sampler(mod)
        else
            K = length(counterfactual_data.y_levels)
            input_size = size(selectdim(counterfactual_data.X, ndims(counterfactual_data.X), 1))
            x = Uniform(extrema(counterfactual_data.X)...)
            y = Categorical(ones(K) ./ K)
            sampler = ConditionalSampler(x, y; input_size=input_size)
        end
        opt = ImproperSGLD()
        f(x) = logits(mod, x)

        n_iter = 200
        _w = 1500
        plts = []
        neach = 10
        for i in 1:10
            x = sampler(f, opt; niter=n_iter, n_samples=neach, y=i)
            plts_i = []
            for j in 1:size(x, 2)
                xj = x[:,j]
                xj = reshape(xj, (n_digits, n_digits))
                plts_i = [plts_i..., Plots.heatmap(rotl90(xj), axis=nothing, cb=false)]
            end
            plt = Plots.plot(plts_i..., size=(_w,0.10*_w), layout=(1,10))
            plts = [plts..., plt]
        end
        plt = Plots.plot(plts..., size=(_w,_w), layout=(10,1), plot_title=mod_name)
        savefig(plt, joinpath(output_images_path, "mnist_generated_$(mod_name).png"))
        display(plt)
    end
end
```

```{julia}
# Evaluate models:

measure = Dict(
    :f1score => multiclass_f1score, 
    :acc => accuracy, 
    :precision => multiclass_precision
)
model_performance = DataFrame()
for (mod_name, mod) in model_dict
    # Test performance:
    test_data = load_mnist_test()
    test_data.X = pre_process.(test_data.X, noise=0.0f0)
    _perf = CounterfactualExplanations.Models.model_evaluation(mod, test_data, measure=collect(values(measure)))
    _perf = DataFrame([[p] for p in _perf], collect(keys(measure)))
    _perf.mod_name .= mod_name
    model_performance = vcat(model_performance, _perf)
end
Serialization.serialize(joinpath(output_path,"mnist_model_performance.jls"), model_performance)
CSV.write(joinpath(output_path, "mnist_model_performance.csv"), model_performance)
model_performance
```

```{julia}
# ECCCo:
位=[0.5,0.1,0.5]
temp=0.5
畏=0.01

# Generate counterfactuals using ECCCo generator:
generator = ECCCoGenerator(
    位=位, 
    temp=temp, 
    opt=Flux.Optimise.Adam(畏),
)

ces = Dict()
for (mod_name, mod) in model_dict
    ce = generate_counterfactual(
        x_factual, target, counterfactual_data, mod, generator; 
        decision_threshold=纬, max_iter=T,
        initialization=:identity,
        converge_when=:generator_conditions,
    )
    ces[mod_name] = ce
end
plt_order = sortperm(collect(keys(ces)))

# Plot:
p1 = Plots.plot(
    convert2image(MNIST, reshape(x_factual,28,28)),
    axis=nothing, 
    size=(img_height, img_height),
    title="Factual"
)

plts = []
for (_name,ce) in ces
    _x = CounterfactualExplanations.counterfactual(ce)
    _phat = target_probs(ce)
    _title = "$_name (p=$(round(_phat[1]; digits=3)))"
    plt = Plots.plot(
        convert2image(MNIST, reshape(_x,28,28)),
        axis=nothing, 
        size=(img_height, img_height),
        title=_title
    )
    plts = [plts..., plt]
end
plts = plts[plt_order]
plts = [p1, plts...]
plt = Plots.plot(plts...; size=(img_height*length(plts),img_height), layout=(1,length(plts)))
display(plt)
savefig(plt, joinpath(output_images_path, "mnist_eccco.png"))
```

```{julia}
Random.seed!(1234)

# Set up search:
factual_label = 9
x_factual = reshape(counterfactual_data.X[:,rand(findall(predict_label(M, counterfactual_data).==factual_label))],input_dim,1)
target = 7
factual = predict_label(M, counterfactual_data, x_factual)[1]
纬 = 0.9
T = 100

畏=1.0

# Generate counterfactual using generic generator:
generator = GenericGenerator(opt=Flux.Optimise.Adam(0.01),)
ce_wachter = generate_counterfactual(
    x_factual, target, counterfactual_data, M, generator; 
    decision_threshold=纬, max_iter=T,
    initialization=:identity,
    converge_when=:generator_conditions,
)

generator = GreedyGenerator(畏=畏)
ce_jsma = generate_counterfactual(
    x_factual, target, counterfactual_data, M, generator; 
    decision_threshold=纬, max_iter=T,
    initialization=:identity,
    converge_when=:generator_conditions,
)

# ECCCo:
位=[0.1,0.1,0.1]
temp=0.1

# Generate counterfactual using ECCCo generator:
generator = ECCCoGenerator(
    位=位, 
    temp=temp, 
    opt=Flux.Optimise.Adam(0.01),
)
ce_conformal = generate_counterfactual(
    x_factual, target, counterfactual_data, M, generator; 
    decision_threshold=纬, max_iter=T,
    initialization=:identity,
    converge_when=:generator_conditions,
)

# Generate counterfactual using ECCCo generator:
generator = ECCCoGenerator(
    位=位, 
    temp=temp, 
    opt=CounterfactualExplanations.Generators.JSMADescent(畏=畏),
)
ce_conformal_jsma = generate_counterfactual(
    x_factual, target, counterfactual_data, M, generator; 
    decision_threshold=纬, max_iter=T,
    initialization=:identity,
    converge_when=:generator_conditions,
)

# Plot:
p1 = Plots.plot(
    convert2image(MNIST, reshape(x_factual,28,28)),
    axis=nothing, 
    size=(img_height, img_height),
    title="Factual"
)
plts = [p1]

ces = [ce_wachter, ce_conformal, ce_jsma, ce_conformal_jsma]
_names = ["Wachter", "ECCCo", "JSMA", "ECCCo-JSMA"]
for x in zip(ces, _names)
    ce, _name = (x[1],x[2])
    x_factual = CounterfactualExplanations.counterfactual(ce)
    _phat = target_probs(ce)
    _title = "$_name (p=$(round(_phat[1]; digits=3)))"
    plt = Plots.plot(
        convert2image(MNIST, reshape(x_factual,28,28)),
        axis=nothing, 
        size=(img_height, img_height),
        title=_title
    )
    plts = [plts..., plt]
end
plt = Plots.plot(plts...; size=(img_height*length(plts),img_height), layout=(1,length(plts)))
display(plt)
savefig(plt, joinpath(www_path, "eccco_mnist.png"))
```

```{julia}
# Random.seed!(1234)

# Set up search:
factual_label = 8
x_factual = reshape(counterfactual_data.X[:,rand(findall(predict_label(M, counterfactual_data).==factual_label))],input_dim,1)
target = 3
factual = predict_label(M, counterfactual_data, x_factual)[1]
纬 = 0.5
T = 100

# Generate counterfactual using generic generator:
generator = GenericGenerator(opt=Flux.Optimise.Adam(),)
ce_wachter = generate_counterfactual(
    x_factual, target, counterfactual_data, M, generator; 
    decision_threshold=纬, max_iter=T,
    initialization=:identity,
)

generator = GreedyGenerator(畏=1.0)
ce_jsma = generate_counterfactual(
    x_factual, target, counterfactual_data, M, generator; 
    decision_threshold=纬, max_iter=T,
    initialization=:identity,
)

# ECCCo:
位=[0.0,1.0]
temp=0.5

# Generate counterfactual using CCE generator:
generator = CCEGenerator(
    位=位, 
    temp=temp, 
    opt=Flux.Optimise.Adam(),
)
ce_conformal = generate_counterfactual(
    x_factual, target, counterfactual_data, M, generator; 
    decision_threshold=纬, max_iter=T,
    initialization=:identity,
    converge_when=:generator_conditions,
)

# Generate counterfactual using CCE generator:
generator = CCEGenerator(
    位=位, 
    temp=temp, 
    opt=CounterfactualExplanations.Generators.JSMADescent(畏=1.0),
)
ce_conformal_jsma = generate_counterfactual(
    x_factual, target, counterfactual_data, M, generator; 
    decision_threshold=纬, max_iter=T,
    initialization=:identity,
    converge_when=:generator_conditions,
)

# Plot:
p1 = Plots.plot(
    convert2image(MNIST, reshape(x_factual,28,28)),
    axis=nothing, 
    size=(img_height, img_height),
    title="Factual"
)
plts = [p1]

ces = [ce_wachter, ce_conformal, ce_jsma, ce_conformal_jsma]
_names = ["Wachter", "CCE", "JSMA", "CCE-JSMA"]
for x in zip(ces, _names)
    ce, _name = (x[1],x[2])
    x = CounterfactualExplanations.counterfactual(ce)
    _phat = target_probs(ce)
    _title = "$_name (p=$(round(_phat[1]; digits=3)))"
    plt = Plots.plot(
        convert2image(MNIST, reshape(x,28,28)),
        axis=nothing, 
        size=(img_height, img_height),
        title=_title
    )
    plts = [plts..., plt]
end
plt = Plots.plot(plts...; size=(img_height*length(plts),img_height), layout=(1,length(plts)))
display(plt)
savefig(plt, joinpath(www_path, "cce_mnist.png"))
```

```{julia}
if M.model.model isa JointEnergyModels.JointEnergyClassifier
    jem = M.model.model.jem
    n_iter = 200
    _w = 1500
    plts = []
    neach = 10
    for i in 1:10
        x = jem.sampler(jem.chain, jem.sampling_rule; niter=n_iter, n_samples=neach, y=i)
        plts_i = []
        for j in 1:size(x, 2)
            xj = x[:,j]
            xj = reshape(xj, (n_digits, n_digits))
            plts_i = [plts_i..., Plots.heatmap(rotl90(xj), axis=nothing, cb=false)]
        end
        plt = Plots.plot(plts_i..., size=(_w,0.10*_w), layout=(1,10))
        plts = [plts..., plt]
    end
    plt = Plots.plot(plts..., size=(_w,_w), layout=(10,1))
    display(plt)
end
```

## Benchmark

```{julia}
# Benchmark generators:
generators = Dict(
    :wachter => GenericGenerator(opt=opt, 位=l2_位),
    :revise => REVISEGenerator(opt=opt, 位=l2_位),
    :greedy => GreedyGenerator(),
)

# Conformal Models: 


# Measures:
measures = [
    CounterfactualExplanations.distance,
    ECCCo.distance_from_energy,
    ECCCo.distance_from_targets,
    CounterfactualExplanations.validity,
]
```