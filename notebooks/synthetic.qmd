```{julia}
include("$(pwd())/notebooks/setup.jl")
eval(setup_notebooks);
```

# Synthetic data

```{julia}
#| output: false

# Data:
datasets = Dict(
    :linearly_separable => load_linearly_separable(),
    :overlapping => load_overlapping(),
    :moons => load_moons(),
    :circles => load_circles(),
    :multi_class => load_multi_class(),
)

# Hyperparameters:
cvgs = [0.5, 0.75, 0.95]
temps = [0.01, 0.1, 1.0]
Λ = [0.0, 0.1, 1.0, 10.0]
l2_λ = 0.1

# Classifiers:
epochs = 250
link_fun = relu
logreg = NeuralNetworkClassifier(builder=MLJFlux.Linear(σ=link_fun), epochs=epochs)
mlp = NeuralNetworkClassifier(builder=MLJFlux.MLP(hidden=(32,), σ=link_fun), epochs=epochs)
ensmbl = EnsembleModel(model=mlp, n=5)
classifiers = Dict(
    # :logreg => logreg,
    :mlp => mlp,
    # :ensmbl => ensmbl,
)

# Search parameters:
target = 2
factual = 1
max_iter = 50
gradient_tol = 1e-2
opt = Descent(0.01)
```

```{julia}
#| echo: false

results = DataFrame()
for (dataname, data) in datasets

    # Data:
    X = table(permutedims(data.X))
    y = data.output_encoder.labels
    x = select_factual(data,rand(1:size(data.X,2)))

    for (clf_name, clf) in classifiers, cov in cvgs

        # Classifier and coverage:
        conf_model = conformal_model(clf; method=:simple_inductive, coverage=cov)
        mach = machine(conf_model, X, y)
        fit!(mach)
        M = ECCCo.ConformalModel(mach.model, mach.fitresult)

        # Set up ECCCo:
        factual_label = predict_label(M, data, x)[1]
        target_label = data.y_levels[data.y_levels .!= factual_label][1]

        for λ in Λ, temp in temps

            # ECCCo for given classifier, coverage, temperature and λ:
            generator = ECCCoGenerator(temp=temp, λ=[l2_λ,λ], opt=opt)
            @assert predict_label(M, data, x) != target_label
            ce = try
                generate_counterfactual(
                    x, target_label, data, M, generator;
                    initialization=:identity,
                    converge_when=:generator_conditions,
                    gradient_tol=gradient_tol,
                    max_iter=max_iter,
                )
            catch
                missing
            end

            _results = DataFrame(
                dataset = dataname,
                classifier = clf_name,
                coverage = cov,
                temperature = temp,
                λ = λ,
                ce = ce,
                factual = factual_label,
                target = target_label,
            )
            append!(results, _results)

        end

    end

end
```

```{julia}
#| echo: false

function plot_ce(results; dataset=:multi_class, classifier=:mlp, λ=0.1, img_height=300, zoom=-0.2)
    df_plot = results[results.dataset .== dataset,:] |>
        res -> res[res.classifier .== classifier,:] |>
        res -> res[res.λ .== λ,:]
    plts = map(eachrow(df_plot)) do row
        Plots.plot(
            row.ce,
            title="cov: $(row.coverage), temp: $(row.temperature)",
            cbar=false,
            zoom=zoom,
            legend=false,
        )
    end
    nrow = length(cvgs)
    ncol = length(temps)
    _layout = (nrow, ncol)
    Plots.plot(
        plts..., 
        size=img_height.*reverse(_layout), layout=_layout,
        plot_title="λ: $λ, dataset: $dataset, classifier: $classifier",
    )
end
```

```{julia}
#| output: true
#| echo: false

for dataset in keys(datasets)
    Markdown.parse("""### $dataset""")
    for classifier in keys(classifiers)
        Markdown.parse("""#### $classifier""")
        Markdown.parse("""::: {.panel-tabset}""")
        for λ in Λ
            Markdown.parse("""##### λ: $λ""")
            display(plot_ce(results; dataset=dataset, classifier=classifier, λ=λ))
        end
        Markdown.parse(""":::""")
    end
end
```

## Benchmark

```{julia}
# Benchmark generators:
generators = Dict(
    :wachter => GenericGenerator(opt=opt, λ=l2_λ),
    :revise => REVISEGenerator(opt=opt, λ=l2_λ),
    :greedy => GreedyGenerator(),
)

# Untrained Models:
models = Dict(Symbol("cov$(Int(100*cov))") => ECCCo.ConformalModel(conformal_model(mlp; method=:simple_inductive, coverage=cov)) for cov in cvgs)

# Measures:
measures = [
    CounterfactualExplanations.distance,
    ECCCo.distance_from_energy,
    ECCCo.distance_from_targets,
    CounterfactualExplanations.validity,
]
```

### Single CE

```{julia}
#| echo: false

_temp = 0.01
results = DataFrame()
for (dataname, data) in datasets

    # Data:
    X = table(permutedims(data.X))
    y = data.output_encoder.labels
    x = select_factual(data,rand(1:size(data.X,2)))

    for (modelname, M) in deepcopy(models)

        # Model training:
        M = train(M, data)
        # Set up ECCCo:
        factual_label = predict_label(M, data, x)[1]
        target_label = data.y_levels[data.y_levels .!= factual_label][1]
    
        for λ in Λ

            # Generators:
            _generators = deepcopy(generators)
            _generators[:cce] = ECCCoGenerator(temp=_temp, λ=[l2_λ,λ], opt=opt)
            _generators[:energy] = ECCCo.EnergyDrivenGenerator(λ=[l2_λ,λ], opt=opt)
            _generators[:target] = ECCCo.TargetDrivenGenerator(λ=[l2_λ,λ], opt=opt)

            for (gen_name, gen) in _generators

                # ECCCo for given models, λ and generator:
                @assert predict_label(M, data, x) != target_label
                ce = try
                    generate_counterfactual(
                        x, target_label, data, M, gen;
                        initialization=:identity,
                        converge_when=:generator_conditions,
                        gradient_tol=gradient_tol,
                        max_iter=max_iter,
                    )
                catch
                    missing
                end

                if !ismissing(ce)
                    eval = DataFrame(evaluate(ce, measure=measures, output_format=:Dict))
                else
                    eval = DataFrame(Dict(Symbol(fun) => missing for fun in measures))
                end

                _results = DataFrame(
                    dataset = dataname,
                    model = modelname,
                    λ = λ,
                    generator = gen_name,
                    ce = ce,
                    factual = factual_label,
                    target = target_label,
                )

                _results = crossjoin(_results, eval; makeunique=true)
                append!(results, _results)

            end
        end
    end
end
```

```{julia}
#| echo: false

function plot_benchmark(results; dataset=:multi_class, modelname=:cov95, img_height=300, zoom=-0.2)
    df_plot = results[results.dataset .== dataset,:] |>
        res -> res[res.model .== modelname,:]
    plts = map(eachrow(df_plot)) do row
        Plots.plot(
            row.ce,
            title="λ: $(row.λ), gen: $(row.generator)",
            cbar=false,
            zoom=zoom,
            legend=false,
        )
    end
    ncol = length(unique(df_plot.generator))
    nrow = length(unique(df_plot.λ))
    _layout = (nrow, ncol)
    Plots.plot(
        plts..., 
        size=img_height.*reverse(_layout), layout=_layout,
        plot_title="dataset: $dataset, model: $modelname",
    )
end
```

```{julia}
#| output: true
#| echo: false

df = @pivot_longer(results, distance:distance_from_targets)
for dataname ∈ sort(unique(df.dataset))
    Markdown.parse("""### $dataname""")
    df_ = df[df.dataset .== dataname, :]
    for model in unique(df_.model)
        Markdown.parse("""#### model: $model""")
        df_plot = df_[df_.model .== model, :]
        df_plot = @mutate(df_plot, lambda = string("λ: ", round(λ, digits=2)))
        plt = AlgebraOfGraphics.data(df_plot) * visual(BarPlot) * 
            mapping(:generator, :value, row=:variable, col=:lambda, color=:generator)
        plt = draw(
            plt, axis=(xlabel="", xticksvisible=false, xticklabelsvisible=false, width=200, height=180), 
            facet=(; linkyaxes=:minimal)
        )   
        # plt.figure[0, :] = Label(
        #     plt.figure, "data: $dataname, model: $model", 
        #     fontsize=20, tellwidth=false
        # )
        display(plt)
    end
end
```

### Full Benchmark

```{julia}
bmks = []
for (dataname, dataset) in datasets
    for λ in Λ, temp in temps
        _generators = deepcopy(generators)
        _generators[:cce] = ECCCoGenerator(temp=temp, λ=[l2_λ,λ], opt=opt)
        _generators[:energy] = ECCCo.EnergyDrivenGenerator(λ=[l2_λ,λ], opt=opt)
        _generators[:target] = ECCCo.TargetDrivenGenerator(λ=[l2_λ,λ], opt=opt)
        bmk = benchmark(
            dataset; 
            models=deepcopy(models), 
            generators=_generators, 
            measure=measures,
            suppress_training=false, dataname=dataname,
            n_individuals=5,
            initialization=:identity,
        )
        bmk.evaluation.λ .= λ
        bmk.evaluation.temperature .= temp
        push!(bmks, bmk)
    end
end
bmk = reduce(vcat, bmks)
```

```{julia}
CSV.write(joinpath(output_path, "synthetic_benchmark.csv"), bmk())
```

```{julia}
#| output: true
#| echo: false

df = bmk()
for dataname ∈ sort(unique(df.dataname))
    Markdown.parse("""### $dataname""")
    df_ = df[df.dataname .== dataname, :]
    for λ in Λ, temp in temps
        Markdown.parse("""#### λ: $λ""")
        df_plot = df_[df_.λ .== λ, :]
        df_plot = df_plot[df_plot.temperature .== temp, :]
        plt = AlgebraOfGraphics.data(df_plot) * visual(BoxPlot) * 
            mapping(:generator, :value, row=:variable, col=:model, color=:generator)
        plt = draw(
            plt, axis=(xlabel="", xticksvisible=false, xticklabelsvisible=false, width=200, height=180), 
            facet=(; linkyaxes=:minimal)
        )   
        display(plt)
    end
end
```
