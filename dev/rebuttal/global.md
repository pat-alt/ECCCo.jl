We would like to thank all of the reviewers for their detailed and thoughtful reviews &mdash; your feedback is truly much appreciated. Below, we will respond to points that have been raised by at least two reviewers. Individual responses to each reviewer contain additional points. 

### Point 1: Add more datasets

We agree that further work could benefit from including additional datasets and will make this point clear in section 7. That being said, we have relied on datasets commonly used in similar studies. Due to the size and scope of this work, we have decided to focus on conveying our motivation, methodology and conclusions through illustrative datasets. 

### Point 2: Add more models

We agree that further work could benefit from including additional models and will make this point clear in section 7. In line with similar studies, we have chosen simple neural network architectures as our starting point. Moving on from there, our goal has been to understand if we can improve these simple models through joint-energy training, in order to yield more plausible counterfactuals that faithfully convey the improved quality of the underlying model. 

To this end, we think that our experiments provide sufficient evidence. The size and scope of this work ultimately led us to prioritise this main point. To get this point across we focused on JEMs, because they are known to have properties that are naturally aligned with the idea of plausible counterfactuals. The question about which other kinds of models yield plausible and faithful counterfactuals (e.g. "MLPs, CNNs, or transformer" but also Bayesian NNs, adversarially trained NNs) is interesting in itself, but something we have delegated to future studies. We will be more clear about this in section 7. 

Nonetheless, to immediately address the reviewers' concerns here, we provide additional qualitative examples for MNIST in the companion PDF. These also include a larger deep ensemble and a simple CCN (LeNet-5), both of which tend to yield more plausible and less noisy counterfactual images than a simple MLP. For comparison, we have also added the corresponding counterfactuals generated by Wachter. In the context of the large ensemble, improved plausibility appears to be driven by better predictive uncertainty quantification. LeNet-5 seems to benefit to some extent from its network architecture that is more appropriate for image data. Wachter fails to uncover any of this. A more detailed study of different models would indeed be very interesting and we believe that ECCCo facilitates such work.

### Point 3: Are ECCCos plausible enough in practice?

We agree that additional qualitative examples for MNIST can help to demonstrate that ECCCo does indeed uncover plausible patterns learned by non-JEM-based classifiers. Based on the reviewers' suggestions we will therefore move the qualitative examples provided in the companion PDF into the supplementary material. Will respect to our other real-world dataset, the results in Table 2 indicate that ECCCo consistently achieves substantially higher plausibility than Wachter. 

It is important to note here, that ECCCo aims to generate faithful counterfactuals first and foremost. Plausibility is achieved only to the extent that the underlying model learns plausible explanations for the data. Thus, we disagree that failure to produce plausible counterfactuals would limit ECCCo's usefulness in practice. We argue that this should not be seen as a weakness, but rather as a strength of ECCCo, for the following reasons:

- For practitioners/researchers it is valuable information indicating that despite good predictive performance, the learned posterior density $p_{\theta}(\mathbf{x}|\mathbf{y^{+}})$ is high in regions of the input domain that are implausible (in the sense of Def 2.1, i.e. the corresponding true density $p(\mathbf{x}|\mathbf{y^{+}})$ is low in those same regions).
- Instead of using surrogate-aided counterfactual search engines to sample those counterfactuals from $p_{\theta}(\mathbf{x}|\mathbf{y^{+}})$ that are indeed plausible, we would argue that the next point of action in such cases should generally be to improve the model.
- We agree that this places an additional burden on researchers/practitioners, but that does not render ECCCo impractical. In situations where providing actionable recourse is an absolute priority, practitioners can always resort to REVISE and related tools in the short term. Major discrepancies between ECCCo and surrogate-aided tools should then at the very least signal to researchers/practitioners, that the underlying model needs to be improved in the medium term.

Based on the reviewers' observations in this context, we will clarify this tension between faithfulness and plausibility further by sharpening the relevant paragraphs in our paper.

### Point 4: Add ablation studies

We already do this to some extent: the experiments involving our synthetic datasets are set up to explicitly address this question where we study *ECCCo (no CP)* and *ECCCo (no EBM)*, respectively. We find that dropping these components generally leads to worse results, but also point out that Conformal Prediction (CP) appears to play less of a role than Energy-Based Modelling (EBM) (lines 278 to 281). We also note in section 7 that further future work is needed to understand the role of CP better (lines 330 to 332). We will expand on this to the extent possible: one possible explanation for the limited impact of CP could be that CP relies on exchangeability. In other words, the smooth set size penalty may not be as effective as intended when we move out of domain during counterfactual search, because it fails to adequately address epistemic uncertainty. Due to the limited size and scope of this work, we have reserved these types of questions for future work.
