
**Title**: ECCCos from the Black Box: Faithful Explanations through Energy-Constrained Conformal Counterfactuals

**Keywords**: Explainable AI, Counterfactual Explanations, Algorithmic Recourse, Energy-Based Models, Conformal Prediction

**Abstract**: (see [paper](paper.pdf))

**Corresponding Author**: p.altmeyer@tudelft.nl 

**Revier Nomination**: Arie.vanDeursen@tudelft.nl

**Primary Area**: Interpretability and Explainability

**Claims**: Yes

**Code of Ethics**: Yes

**Broader Impacts**: Any work on Explainable AI should be conducted by carefully considering its potential societal impacts. We take this very seriously by formalising the need for Counterfactual Explanations to faithfully describe model behaviour. We argue that a narrow focus on generating plausible counterfactuals may lead practitioners and researchers to believe that even a highly vulnerable black-box model has learned plausible representations of the data. This is precisely the kind of broader societal impact we aim to mitigate through our work. 

**Limitations**: Yes

**Theory**: n/a

**Experiments**: Yes

**Training Details**: Yes

**Error Bars**: Yes

**Compute**: ?

**Reproducibility**: Yes

**Safeguards**: n/a

**Licenses**: Yes

**Assets**: Yes

**Human Subjects**: n/a

**IRB Approvals**: n/a

